{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capstone Week 4\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Index\n",
    "- [Capstone Objectives](#Capstone-Objectives)\n",
    "- [Read in Data](#Read-in-Data)\n",
    "    - [Merge 2018 and 2019](#Merge-2018-and-2019)\n",
    "    - [Make advisor and firm dictionary mapper](#Make-advisor-and-firm-dictionary-mapper)\n",
    "- [EDA](#EDA)\n",
    "- [Data Cleaning](#Data-Cleaning)\n",
    "    - [Train-Test-Split](#Train-Test-Split)\n",
    "- [Missing Data](#Missing-Data)\n",
    "    - [How big of a problem is missing data?](#How-big-of-a-problem-is-missing-data?)\n",
    "    - [Three types of missing data](#Three-types-of-missing-data)\n",
    "    - [Strategies for handling missing data](#Strategies-for-handling-missing-data)\n",
    "        - [Weight Class Adjustment Example](#Weight-Class-Adjustment-Example)\n",
    "    - [Imputation Strategies](#Imputation-Strategies)\n",
    "    - [Missingness Tests](#Missingness-Tests)\n",
    "    - [MCAR Data](#MCAR-Data)\n",
    "    - [MAR Data](#MAR-Data)\n",
    "    - [NMAR Data](#NMAR-Data)\n",
    "    - [Missing data workflow](#Missing-data-workflow)\n",
    "    - [Custom Cleaning Functions](#Custom-Cleaning-Functions)\n",
    "    - [Create Cleaning Pipeline](#Create-Cleaning-Pipeline)\n",
    "- [Model building](#Model-building)\n",
    "- [Make predictions](#Make-predictions)\n",
    "- [Feature Engineering](#Feature-Engineering)\n",
    "    - [Variable Inflation Factor (VIF)](#Variable-Inflation-Factor-(VIF))\n",
    "- [Residuals](#Residuals)\n",
    "- [Classification](#Classification)\n",
    "- [Model Interpretation](#Model-Interpretation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capstone Objectives\n",
    "- Assist sales and marketing by improving their targeting\n",
    "- Predict sales for 2019 using the data for 2018\n",
    "- Estimate the probability of adding a new fund in 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\n",
    "\n",
    "pd.set_option('display.max_columns', 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to Top](#Index)\n",
    "# Read in Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../../capstone-run2/Transaction Data.xlsx'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-869a4fe78193>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf18\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_excel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../../capstone-run2/Transaction Data.xlsx\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msheet_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Transactions18'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf19\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_excel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../../capstone-run2/Transaction Data.xlsx\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msheet_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Transactions19'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mfirm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_excel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../../capstone-run2/Firm Information.xlsx\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msheet_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Rep summary\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/tf2/lib/python3.7/site-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    297\u001b[0m                 )\n\u001b[1;32m    298\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFutureWarning\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 299\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    300\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/tf2/lib/python3.7/site-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36mread_excel\u001b[0;34m(io, sheet_name, header, names, index_col, usecols, squeeze, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, parse_dates, date_parser, thousands, comment, skipfooter, convert_float, mangle_dupe_cols, storage_options)\u001b[0m\n\u001b[1;32m    334\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExcelFile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m         \u001b[0mshould_close\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 336\u001b[0;31m         \u001b[0mio\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mExcelFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    337\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m         raise ValueError(\n",
      "\u001b[0;32m/opt/anaconda3/envs/tf2/lib/python3.7/site-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, path_or_buffer, engine, storage_options)\u001b[0m\n\u001b[1;32m   1070\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1071\u001b[0m                 ext = inspect_excel_format(\n\u001b[0;32m-> 1072\u001b[0;31m                     \u001b[0mcontent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1073\u001b[0m                 )\n\u001b[1;32m   1074\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/tf2/lib/python3.7/site-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36minspect_excel_format\u001b[0;34m(path, content, storage_options)\u001b[0m\n\u001b[1;32m    948\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m     with get_handle(\n\u001b[0;32m--> 950\u001b[0;31m         \u001b[0mcontent_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_text\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    951\u001b[0m     ) as handle:\n\u001b[1;32m    952\u001b[0m         \u001b[0mstream\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/tf2/lib/python3.7/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    649\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    650\u001b[0m             \u001b[0;31m# Binary mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 651\u001b[0;31m             \u001b[0mhandle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    652\u001b[0m         \u001b[0mhandles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    653\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../../capstone-run2/Transaction Data.xlsx'"
     ]
    }
   ],
   "source": [
    "df18 = pd.read_excel(\"../../capstone-run2/Transaction Data.xlsx\", sheet_name='Transactions18')\n",
    "df19 = pd.read_excel(\"../../capstone-run2/Transaction Data.xlsx\", sheet_name='Transactions19')\n",
    "firm = pd.read_excel(\"../../capstone-run2/Firm Information.xlsx\", sheet_name=\"Rep summary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge 2018 and 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.merge(\n",
    "    df18,\n",
    "    df19,\n",
    "    on='CONTACT_ID',\n",
    "    suffixes=['_2018', '_2019']\n",
    ")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make advisor and firm dictionary mapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adviser_lookup = {\n",
    "    idx: contact_id \n",
    "        for idx, contact_id in enumerate(df['CONTACT_ID'])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adviser_lookup[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "firm_lookup = {idx: contact_id for idx, contact_id in enumerate(firm['Contact ID'])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "firm_lookup[10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to Top](#Index)\n",
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !conda install -yc conda-forge pandas-profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pandas_profiling import ProfileReport\n",
    "\n",
    "# missing_diagrams = {\n",
    "#     'heatmap': True, 'dendrogram': True, 'matrix':True, 'bar': True,\n",
    "# }\n",
    "\n",
    "# profile = ProfileReport(df, title='Nuveen Profile Report', missing_diagrams=missing_diagrams)\n",
    "\n",
    "# profile.to_file(output_file=\"nuveen_profiling.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to Top](#Index)\n",
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a variable to keep all of the columns we want to drop\n",
    "COLS_TO_DROP = [\n",
    "    'refresh_date_2019', 'refresh_date_2018', 'CONTACT_ID',\n",
    "    'Contact ID', 'CustomerID', 'Firm ID', 'Office ID',\n",
    "    'Channel','Sub channel', 'Firm name'\n",
    "]\n",
    "\n",
    "COLS_TO_KEEP = [\n",
    "    'no_of_sales_12M_1', 'no_of_Redemption_12M_1', 'no_of_sales_12M_10K',\n",
    "    'no_of_Redemption_12M_10K', 'no_of_funds_sold_12M_1',\n",
    "    'no_of_funds_redeemed_12M_1', 'no_of_fund_sales_12M_10K',\n",
    "    'no_of_funds_Redemption_12M_10K', 'no_of_assetclass_sold_12M_1',\n",
    "    'no_of_assetclass_redeemed_12M_1', 'no_of_assetclass_sales_12M_10K',\n",
    "    'no_of_assetclass_Redemption_12M_10K', 'No_of_fund_curr',\n",
    "    'No_of_asset_curr', 'AUM', 'sales_curr', 'sales_12M_2018',\n",
    "    'redemption_curr', 'redemption_12M', 'new_Fund_added_12M_2018',\n",
    "    'aum_AC_EQUITY', 'aum_AC_FIXED_INCOME_MUNI',\n",
    "    'aum_AC_FIXED_INCOME_TAXABLE', 'aum_AC_MONEY', 'aum_AC_MULTIPLE',\n",
    "    'aum_AC_PHYSICAL_COMMODITY', 'aum_AC_REAL_ESTATE', 'aum_AC_TARGET',\n",
    "    'aum_P_529', 'aum_P_ALT', 'aum_P_CEF', 'aum_P_ETF', 'aum_P_MF',\n",
    "    'aum_P_SMA', 'aum_P_UCITS', 'aum_P_UIT', \n",
    "]\n",
    "\n",
    "FIRM_COLS = ['Contact ID', 'Channel','Sub channel',]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make `Firm` data pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.merge(df, firm, left_on=\"CONTACT_ID\", right_on='Contact ID')\n",
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(['sales_12M_2019', 'new_Fund_added_12M_2019'], axis=1)\n",
    "y_reg = df['sales_12M_2019']\n",
    "y_cl = df['new_Fund_added_12M_2019']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-Test-Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train_reg, y_test_reg = train_test_split(\n",
    "    X, y_reg, test_size=0.3, random_state=24\n",
    ")\n",
    "y_train_cl, y_test_cl = y_cl[y_train_reg.index], y_cl[y_test_reg.index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to Top](#Index)\n",
    "# Missing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Up to this point we haven't put much thought into dealing with missing data. Missing data is EVERYWHERE and it's important to know how to do data science with missing data. It can significantly undermine our results!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How big of a problem is missing data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is difficult question because we only see what we observe. We can use simulated data to help answer this question, but we cannot quantify the impact of missing data in our real data projects.\n",
    "\n",
    "See this resource: https://github.com/matthewbrems/jupytercon-missing-data-2018"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Three types of missing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **MCAR**: Missing Completely at Random\n",
    "    - Some intern spills their coffee on your surveys in random order\n",
    "    - Flip coin of missingness\n",
    "    \n",
    "    \n",
    "2. **MAR**: Missing at Random\n",
    "    - I adminster a survey about income. Those who are female are less likely to respond to the question about income.\n",
    "    - Missing data is conditional on data have observed.\n",
    "\n",
    "\n",
    "3. **NMAR**: Not Missing at Random (Worst type!)\n",
    "    - I adminster a survey that includes a question about income. Those who have lower incomes are less likely to respond to the question about income.\n",
    "    - Data of interest are systematically different for respondents and nonrespondents\n",
    "    - Whether the data are missing or not depends on the value of the unobserved value itself!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to Top](#Index)\n",
    "### Strategies for handling missing data\n",
    "1. **Avoid it** (best option, if possible)\n",
    "    - Use sound design when collecting data\n",
    "    - Improve survey questioning and design\n",
    "    - Drop all rows with _any_ missing value\n",
    "    \n",
    "    \n",
    "2. **Ignore it** (second best option, if possible): \n",
    "    - Assume your respondents are close enough to the sample of non-respondents\n",
    "    - Drop any observation with _any_ missing value\n",
    "    \n",
    "    \n",
    "3. **Account for it** (most common):\n",
    "    - Weight class adjustments\n",
    "    - Determine why data are missing\n",
    "    - Employ a strategy for accounting for missing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to Top](#Index)\n",
    "#### Weight Class Adjustment Example\n",
    "\n",
    "I'm estimating job satisfaction among two departments: finance and accounting. Both departments are the same size (A: 50%, F: 50%).\n",
    "\n",
    "$$W_{finance} = \\frac{true\\;proportion}{proportion\\;of\\;responses} = \\frac{0.50}{0.25} = 2$$\n",
    "<br>\n",
    "$$W_{accounting} = \\frac{true\\;proportion}{proportion\\;of\\;responses} = \\frac{0.50}{0.75} = \\frac{2}{3}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to Top](#Index)\n",
    "### Imputation Strategies\n",
    "\n",
    "1. Deductive Imputation: use logical relationships to fill in value **VALID**\n",
    "\n",
    "    - Respondent says the were not victim of crime, but left \"victim of a violent crime\" question blank.\n",
    "    - If someone has 2 children in year 1, `NaN` children in year 2, and 2 children in year 3, we can _probably_ impute that in year 2 they still had 2 children.\n",
    "    - PRO: Valid method, requires minimal \"inference\"\n",
    "    - CON: Time consuming and requires specific coding\n",
    "\n",
    "\n",
    "2. Mean/Median/Mode: use measure central tendency to fill value **INVALID**\n",
    "\n",
    "     - PRO: Easy to implement\n",
    "     - CON: Significantly distorts histogram (underestimates variance) and results will look more precise than they really are\n",
    "     \n",
    "\n",
    "3. Regression Imputation: replace missing based on predicted value from regression line **INVALID**\n",
    "\n",
    "    - PRO: Easy to understand\n",
    "    - CON: Distorts distribution and underestimates variance still because there is no randomness in the prediction\n",
    "    \n",
    "    \n",
    "4. Stochastic Regression Imputation:\n",
    "\n",
    "    - Replace missing with predicted value from regression line plus random draw from normal distribution `N(0, s)`, where `s` is estimataed from model residuals **INVALID**\n",
    "    \n",
    "    - PRO: Easy to understand and better than just regression technique\n",
    "    - CON: Still under estimate variance because selecting single point from normal distribution of error\n",
    "    \n",
    "    \n",
    "5. Multiply Stochastic Regression Imputation: pull multiple values from distribution. Replace missing with predicted value from line with random error.\n",
    "\n",
    "    - PRO: Better than number 4\n",
    "    - CON: All `Beta` coefficients are constant, so still not credible\n",
    "    \n",
    "    \n",
    "6. Proper Multiply Stochastic Regression Imputation: Called Multiple Imputation by Chained Equations [(MICE)](https://stats.stackexchange.com/questions/421545/multiple-imputation-by-chained-equations-mice-explained)\n",
    "\n",
    "    - Create `n` copies of your data set (let's say, 10)\n",
    "    - For each dataset:\n",
    "        - Generate coefficients for your regression model\n",
    "            - For each missing value:\n",
    "                - Replace `NaN` with a value predicted from a regression\n",
    "            - Do your \"final analysis\" or generate your final prediction\n",
    "    - Aggregate your analysis/predictions across all data sets so you have one complete analysis\n",
    "    - These predictions were created by properly estimating the variance in your data\n",
    "    - PRO: Very good method, **VALID**\n",
    "    - CON: Takes more effort to implement (`fancyimpute` or `mice` in R)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to Top](#Index)\n",
    "### Missingness Tests\n",
    "\n",
    "1. Little's Test for MCAR\n",
    "    - $H_0 : MCAR$\n",
    "    - $H_A : MAR$\n",
    "    - There is no test for NMAR!\n",
    "2. Split your data into \"observed\" and \"unobserved\" and compare them\n",
    "    - Split missing `income` and observed `income` sets. Do the other variables have the same distributions?\n",
    "3. Think about missing data process. Can you come up with a reasonable answer based on how missing data came about?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to Top](#Index)\n",
    "### MCAR Data\n",
    "\n",
    "Use any of the methods we previously discussed:\n",
    "- Deductive imputation\n",
    "- Proper imputation\n",
    "- Stochastic Regression Imputation\n",
    "- Complete-Case Removal (unbiased, but variance will be higher because our sample size is smaller!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MAR Data\n",
    "\n",
    "Use one of the following methods:\n",
    "- Deductive imputation\n",
    "- Proper imputation\n",
    "- Stochastic Regression Imputation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NMAR Data\n",
    "\n",
    "Use one of the following methods:\n",
    "- Deductive imputation\n",
    "- Advanced methods: selection models and pattern mixture models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to Top](#Index)\n",
    "### Missing data workflow\n",
    "1. How much missing data do I have?\n",
    "2. For each variable, estimate the type of missing data\n",
    "3. What is the best method for handling missing values?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Cleaning Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create functions that do some basic housekeeping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_columns(df):\n",
    "    '''extract out columns not listed in COLS_TO_DROP variable'''\n",
    "    cols_to_keep = [col for col in df.columns if col not in COLS_TO_DROP]\n",
    "    return df.loc[:, cols_to_keep].copy()\n",
    "\n",
    "\n",
    "def fillna_values(df):\n",
    "    '''fill nan values with zero'''\n",
    "    if isinstance(df, type(pd.Series(dtype='float64'))):\n",
    "        return df.fillna(0)\n",
    "    num_df = df.select_dtypes(include=['number']).fillna(0)\n",
    "    non_num_df = df.select_dtypes(exclude=['number'])\n",
    "    return pd.concat([num_df, non_num_df], axis=1)\n",
    "\n",
    "\n",
    "def negative_to_zero(df):\n",
    "    if isinstance(df, type(pd.Series(dtype='float64'))):\n",
    "        return df.apply(lambda x: max(0, x))\n",
    "    else:\n",
    "        return df.select_dtypes(include='number').clip(lower=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to Top](#Index)\n",
    "## Create Cleaning Pipeline\n",
    "\n",
    "- Pipeline for target variable\n",
    "- Pipeline for features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer, PowerTransformer\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_columns_trans = FunctionTransformer(extract_columns)\n",
    "fillna_values_trans = FunctionTransformer(fillna_values)\n",
    "negative_to_zero_trans = FunctionTransformer(negative_to_zero)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make pipeline for regression target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targ_pipe_reg = Pipeline([\n",
    "    ('fillna_values_trans', fillna_values_trans),\n",
    "    ('negative_to_zero_trans', negative_to_zero_trans),\n",
    "    ('PowerTransformer', PowerTransformer(standardize=False))\n",
    "])\n",
    "\n",
    "y_train_reg = pd.Series(\n",
    "    targ_pipe_reg.fit_transform(y_train_reg.to_frame()).squeeze(),\n",
    "    index=y_train_reg.index\n",
    ")\n",
    "y_test_reg = pd.Series(\n",
    "    targ_pipe_reg.transform(y_test_reg.to_frame()).squeeze(),\n",
    "    index=y_test_reg.index\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_reg.hist(bins=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform the classification target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Binarizer\n",
    "\n",
    "targ_pipe_cl = Pipeline([\n",
    "    ('fillna_values_trans', fillna_values_trans),\n",
    "    ('Binarizer', Binarizer(threshold=0))\n",
    "])\n",
    "\n",
    "y_train_cl = pd.Series(\n",
    "    targ_pipe_cl\n",
    "        .fit_transform(y_train_cl.to_frame())\n",
    "        .reshape(-1), index=y_train_cl.index)\n",
    "\n",
    "y_test_cl = pd.Series(\n",
    "    targ_pipe_cl\n",
    "        .transform(y_test_cl.to_frame())\n",
    "        .reshape(-1), index=y_test_cl.index)\n",
    "y_test_cl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the pipeline for the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_pipe = Pipeline([\n",
    "    ('extract_columns_trans', extract_columns_trans),\n",
    "    ('fillna_values_trans', fillna_values_trans),\n",
    "    ('StandardScaler', StandardScaler()),\n",
    "])\n",
    "\n",
    "X_train_prepared = feat_pipe.fit(X_train).transform(X_train)\n",
    "X_test_prepared = feat_pipe.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TRANSFORM** Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_prepared = pd.DataFrame(\n",
    "    X_train_prepared,\n",
    "    index=X_train.index,\n",
    "    columns=COLS_TO_KEEP\n",
    ")\n",
    "\n",
    "X_test_prepared = pd.DataFrame(\n",
    "    feat_pipe.transform(X_test),\n",
    "    index=X_test.index,\n",
    "    columns=COLS_TO_KEEP\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_prepared"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to Top](#Index)\n",
    "# Model building\n",
    "- Evaluate baseline model\n",
    "- Create new models\n",
    "- Create evaluation function and cross validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression()\n",
    "lr.fit(X_train_prepared, y_train_reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "-cross_validate(\n",
    "    lr, \n",
    "    X_train_prepared, \n",
    "    y_train_reg, \n",
    "    cv=3, \n",
    "    scoring='neg_root_mean_squared_error', \n",
    "    return_train_score=True\n",
    ")['test_score']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a plot of predictions vs actual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_reg_preds = lr.predict(X_test_prepared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "axes.scatter(x=y_test_reg, y=y_test_reg_preds)\n",
    "\n",
    "axes.plot([0, 20000000], [0,20000000])\n",
    "axes.set_title(\"Actual vs Predicted - Regression\")\n",
    "axes.set_xlabel(\"Actual\")\n",
    "axes.set_ylabel(\"Predicted\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X, y):\n",
    "    print(\"Cross Validation Scores:\")\n",
    "    print(-cross_validate(model, X, y, scoring='neg_root_mean_squared_error')['test_score'])\n",
    "    print('-'*55)\n",
    "    preds = model.predict(X)\n",
    "    lim = max(preds.max(), y.max())\n",
    "    fig, ax = plt.subplots(1,1,figsize=(7,5))\n",
    "    ax.scatter(x=y, y=preds, alpha=0.4)\n",
    "    ax.plot([0, lim], [0, lim])\n",
    "    ax.set_xlim([0, lim])\n",
    "    ax.set_ylim([0, lim])\n",
    "    ax.set_title(\"Actual vs Predicted - Regression\")\n",
    "    ax.set_xlabel(\"Actual\")\n",
    "    ax.set_ylabel(\"Predicted\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(lr, X_test_prepared, y_test_reg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make function to output deciles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_deciles(model, X, y):\n",
    "    results = pd.DataFrame(model.predict(X), index=X.index, columns=['predictions'])\n",
    "    results['actual'] = y.values\n",
    "    results['deciles'] = pd.qcut(results['predictions'], 10, labels=False)\n",
    "    results['contact_id'] = results.index.map(adviser_lookup)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regression_deciles = output_deciles(lr, X_test_prepared, y_test_reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regression_deciles.groupby('deciles')[['actual']].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to Top](#Index)\n",
    "## Residual Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_reg_preds = lr.predict(X_test_prepared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the residuals\n",
    "residuals = y_test_reg_preds - y_test_reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot predictions vs residuals\n",
    "fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(14,10))\n",
    "\n",
    "# plot scatter plot on upper left plot\n",
    "axes[0,0].scatter(x=y_test_reg_preds, y=residuals, alpha=0.5)\n",
    "axes[0,0].set(xlabel='Predictions', ylabel='Residuals')\n",
    "\n",
    "# plot a hist on upper right plot\n",
    "axes[0,1].hist(residuals, bins=50)\n",
    "axes[0,1].set(xlabel='Residuals', ylabel='Frequency');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.api import qqplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qqplot(residuals, fit=True, line='r', ax=axes[1,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_reg_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targ_pipe_reg.named_steps['PowerTransformer'].inverse_transform(y_test_reg_preds.reshape(-1,1)).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install scikit-plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scikitplot as skplt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_classifier(X, y, model):\n",
    "    pass\n",
    "    # print classification report\n",
    "    # create lift charts\n",
    "    # create gains charts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf.fit(X_train_prepared, y_train_cl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_cl_preds = rf.predict_proba(X_test_prepared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skplt.metrics.plot_lift_curve(y_test_cl, y_test_cl_preds);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sort(y_test_cl_preds)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Classifcation Deciles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_deciles_class(model, X, y):\n",
    "    results = pd.DataFrame(model.predict_proba(X)[:, 1], index=X.index, columns=['predictions'])\n",
    "    results['actual'] = y.values\n",
    "    results['deciles'] = pd.qcut(results['predictions'], 10, labels=False)\n",
    "    results['contact_id'] = results.index.map(adviser_lookup)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_results = output_deciles_class(rf, X_test_prepared, y_test_cl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_results.groupby('deciles')[['actual']].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
