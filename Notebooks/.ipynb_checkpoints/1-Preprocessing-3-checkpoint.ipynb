{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capstone Week 1\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Capstone Objectives\n",
    "- Assist sales and marketing by improving their targeting, and allowing them to correctly assign coverage channels\n",
    "- Predict sales for 2020 using the data for 2018 and 2019\n",
    "    - Determine the optimal number of models that need to be built\n",
    "    - Develop several regression analyses and show the lift that will be achieved through your models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\n",
    "\n",
    "pd.set_option('display.max_columns', 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remember the data science workflow\n",
    "<img src=\"../data_science_workflow.png\" width=\"400px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/thomas/Documents/GitHub/Columbia-Capstone/Notebooks'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Dictionary.xlsx            \u001b[34mNotebooks\u001b[m\u001b[m/\r\n",
      "Firm Information.xlsx           README.md\r\n",
      "LICENSE                         Transaction_Data_20210128.xlsx\r\n",
      "Lift Chart Template.xlsx\r\n"
     ]
    }
   ],
   "source": [
    "ls ../"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(\n",
    "    '../Transaction_Data_20210128.xlsx', \n",
    "    sheet_name='36 Rolling Months',\n",
    "    usecols='B:AN',\n",
    "    header=1,\n",
    "    skiprows=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('Transactions.csv', drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "firms = pd.read_excel(\n",
    "    \"../Firm Information.xlsx\", \n",
    "    sheet_name='Rep summary', \n",
    "    header=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "firms.to_csv('Fir')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "firms.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Look at the data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Look at the `refresh_date` column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['refresh_date'].describe(datetime_is_numeric=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Look at number of rows and unique advisors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['CONTACT_ID'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['CONTACT_ID'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine `sales_curr` and `sales_12M`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['total_sales'] = df['sales_curr'] + df['sales_12M']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you change ANYTHING with the data - besides the above :) - do your train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURES = [\n",
    "    'CONTACT_ID', 'no_of_sales_12M_1', 'no_of_Redemption_12M_1',\n",
    "    'no_of_sales_12M_10K', 'no_of_Redemption_12M_10K',\n",
    "    'no_of_funds_sold_12M_1', 'no_of_funds_redeemed_12M_1',\n",
    "    'no_of_fund_sales_12M_10K', 'no_of_funds_Redemption_12M_10K',\n",
    "    'no_of_assetclass_sold_12M_1', 'no_of_assetclass_redeemed_12M_1',\n",
    "    'no_of_assetclass_sales_12M_10K', 'no_of_assetclass_Redemption_12M_10K',\n",
    "    'No_of_fund_curr', 'No_of_asset_curr', 'AUM', 'sales_curr', 'sales_12M',\n",
    "    'redemption_curr', 'redemption_12M', 'new_Fund_added_12M',\n",
    "    'redemption_rate', 'aum_AC_EQUITY', 'aum_AC_FIXED_INCOME_MUNI',\n",
    "    'aum_AC_FIXED_INCOME_TAXABLE', 'aum_AC_MONEY', 'aum_AC_MULTIPLE',\n",
    "    'aum_AC_PHYSICAL_COMMODITY', 'aum_AC_REAL_ESTATE', 'aum_AC_TARGET',\n",
    "    'aum_P_529', 'aum_P_ALT', 'aum_P_CEF', 'aum_P_ETF', 'aum_P_MF',\n",
    "    'aum_P_SMA', 'aum_P_UCITS', 'aum_P_UIT', 'refresh_date',\n",
    "]\n",
    "TARGETS = 'total_sales'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partition training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['refresh_date'].dt.year.isin([2017, 2018, 2019])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_rows = df['refresh_date'].dt.year.isin([2017, 2018, 2019])\n",
    "testing_rows = df['refresh_date'].dt.year.isin([2020])\n",
    "\n",
    "X = df.loc[training_rows, FEATURES].copy()\n",
    "y = df.loc[training_rows, TARGETS].copy()\n",
    "\n",
    "y_holdout_test = df.loc[testing_rows, TARGETS].copy() # forget about this for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create functions that do some basic housekeeping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a variable to keep all of the columns we want to drop\n",
    "COLS_TO_DROP = [\n",
    "    'CONTACT_ID', 'sales_curr', 'sales_12M', \n",
    "    'refresh_date', 'new_Fund_added_12M','no_of_Redemption_12M_1',\n",
    "]\n",
    "\n",
    "COLS_TO_KEEP = [\n",
    "    'no_of_sales_12M_1', \n",
    "    'no_of_sales_12M_10K', 'no_of_Redemption_12M_10K',\n",
    "    'no_of_funds_sold_12M_1', 'no_of_funds_redeemed_12M_1',\n",
    "    'no_of_fund_sales_12M_10K', 'no_of_funds_Redemption_12M_10K',\n",
    "    'no_of_assetclass_sold_12M_1', 'no_of_assetclass_redeemed_12M_1',\n",
    "    'no_of_assetclass_sales_12M_10K', 'no_of_assetclass_Redemption_12M_10K',\n",
    "    'No_of_fund_curr', 'No_of_asset_curr', 'AUM', 'redemption_curr', \n",
    "    'redemption_12M', 'redemption_rate', 'aum_AC_EQUITY', \n",
    "    'aum_AC_FIXED_INCOME_MUNI', 'aum_AC_FIXED_INCOME_TAXABLE', 'aum_AC_MONEY', \n",
    "    'aum_AC_MULTIPLE', 'aum_AC_PHYSICAL_COMMODITY', 'aum_AC_REAL_ESTATE', \n",
    "    'aum_AC_TARGET', 'aum_P_529', 'aum_P_ALT', 'aum_P_CEF', 'aum_P_ETF', \n",
    "    'aum_P_MF', 'aum_P_SMA', 'aum_P_UCITS', 'aum_P_UIT',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_columns(df):\n",
    "    '''extract out columns not listed in COLS_TO_DROP variable'''\n",
    "    cols_to_keep = [col for col in df.columns if col not in COLS_TO_DROP]\n",
    "    return df.loc[:, cols_to_keep].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train1 = extract_columns(X_train)\n",
    "X_train1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check out missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train1.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make function to fill those missing values with `0`. We can refactor this function later if we like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.select_dtypes(include=['number'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fillna_values(df):\n",
    "    '''fill nan values with zero'''\n",
    "    if isinstance(df, type(pd.Series(dtype='float64'))):\n",
    "        return df.fillna(0)\n",
    "    num_df = df.select_dtypes(include=['number']).fillna(0)\n",
    "    non_num_df = df.select_dtypes(exclude=['number'])\n",
    "    return pd.concat([num_df, non_num_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train2 = fillna_values(X_train1)\n",
    "y_train = fillna_values(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss = StandardScaler()\n",
    "X_train3 = pd.DataFrame(\n",
    "    ss.fit_transform(X_train2),\n",
    "    index=X_train2.index,\n",
    "    columns=X_train2.columns\n",
    ")\n",
    "X_train3.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Pipeline\n",
    "1. `extract_columns` to drop unneeded columns\n",
    "2. `fillna_values` to replace nans with 0\n",
    "3. Standardize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "extract_columns_trans = FunctionTransformer(extract_columns)\n",
    "fillna_values_trans = FunctionTransformer(fillna_values)\n",
    "\n",
    "fillna_values_trans.transform(X_train).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fillna_values_trans.transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fillna_values_trans.transform(X_train).head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('extract_columns_trans', extract_columns_trans),\n",
    "    ('fillna_values_trans', fillna_values_trans),\n",
    "    ('StandardScaler', StandardScaler()),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train4 = pd.DataFrame(\n",
    "    pipe.fit(X_train, y_train).transform(X_train),\n",
    "    index=X_train.index,\n",
    "    columns=COLS_TO_KEEP\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train4.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test4 = pd.DataFrame(\n",
    "    pipe.transform(X_test),\n",
    "    index=X_test.index,\n",
    "    columns=COLS_TO_KEEP\n",
    ")\n",
    "X_test4.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import cross_validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy = DummyRegressor()\n",
    "dummy.fit(X_train4, y_train)\n",
    "dummy.score(X_train4, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add a model to the end of the Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([\n",
    "    ('extract_columns_trans', extract_columns_trans),\n",
    "    ('fillna_values_trans', fillna_values_trans),\n",
    "    ('ss', StandardScaler()),\n",
    "    ('pca', PCA(n_components=5)),\n",
    "    ('lr', LinearRegression())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_preds = pipe.predict(X_test)\n",
    "y_test_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(figsize=(8,6))\n",
    "axes.scatter(y_test.fillna(0), y_test_preds);\n",
    "axes.set_xlabel(\"True labels\")\n",
    "axes.set_ylabel(\"Predicted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
